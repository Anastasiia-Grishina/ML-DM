{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab2: Binary classification and model evaluation\n",
    "---\n",
    "\n",
    "Result of that lab is a **report** in ipynb format. This course is not about coding, so the less code you write the better. It is easier to find mistakes and expand or modify experiments.\n",
    "![E=(mc)^2 : errors = (more code)^2](https://pp.userapi.com/c638722/v638722272/1f4b3/J0mqkFTF0IY.jpg)\n",
    "\n",
    "Try to write you report as interesting story by consequently answer questions from the task. \n",
    "\n",
    "**($\\star$)** questions add no extra points to assessment, but boost your skills and karma a little bit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Spam classification\n",
    "\n",
    "Let's try to build machine learning models to divide spam from non-spam. \n",
    "\n",
    "Dataset: [UCI](https://archive.ics.uci.edu/ml/datasets.html)\n",
    "\n",
    "Each object in dataset is a letter with features based on text, spam is a positive example, normal letter - negative.\n",
    "\n",
    "#### Task\n",
    "   - Download dataset [Spambase](https://archive.ics.uci.edu/ml/datasets/Spambase) . (Code is provided bellow)\n",
    "   - How many letters in a dataset ?\n",
    "   - Which portion of them is bad (spam) ?\n",
    "   - How you can group letters' features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Classifier training and  it's evaluation\n",
    "\n",
    "We want our model to generalize well. That means to predict class on the data that we did not see during training phase.\n",
    "In order to achive this model should be trained and evaluated on independent sets of examples. Usually, we divide our dataset to two subsets: `train` and `test`. (sometimes we need to split on 3 sets `train`, `validation`, `test`). How to split the data is a compromise: bigger `train` gives you more information and you can build <font color='red'>_better_</font>  algorithms, on the other hand more control examples from `test` would give you less noisy estimate of model quality. \n",
    "\n",
    "For model evaluation you will analyse [confusion matrix](http://en.wikipedia.org/wiki/Confusion_matrix): Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class. \n",
    "\n",
    "![Confusion matrix](http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png)\n",
    "\n",
    "Diagonal consists of correctly classified positive (True positive - TP) and negative (TN) examples. There are two types of errors. False Positive (FP) — type I erros (false activation, good letter went to spam), False Negative (FN) — type II errors (did not filter span). <font color='red'>**Errors of type I and II could have different cost.**</font>\n",
    "\n",
    "<img src=\"https://pp.userapi.com/c837427/v837427272/130f4/PJX8E_FvkG8.jpg\" alt=\"Types of errors\" style=\"width: 300px;\"/>\n",
    "\n",
    "For binary classification we have following quality metrics:\n",
    "  - Accuracy = (TP + TN) / (TP + TN + FP + FN)  — _fraction of correct predictions_\n",
    "  - Precision = TP / (TP + FP)  — _accuracy, fraction of real spam in the letters classified as spam_\n",
    "  - Recall = TP / (TP + FN)  — _completeness, fraction of filtered spam_\n",
    "  - F1 = 2TP / (2TP + FP + FN)  — _harmonic mean of precision and recall_\n",
    "  \n",
    "More information you can find here: [Precision and Recall](http://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "#### Task \n",
    "   - Split dataset into to disjoint subsets: `train` - first 3000 examples (≈65%), `test` - all others\n",
    "   - Train decesion tree with `train`. Classify examples from `test`. Calculate classification quality metrics, described above: Accuracy, Precision, Recall, F1.  [Recommended parameters: split criteria - gini, max_depth: 7]\n",
    "   - Which drawbacks does evaluation on this `test` have? How you can make evalution more informative?\n",
    "   - Repeat experiment with dataset shuffled before `split`.\n",
    "   - Which features are the most informative? Use feature importance from DecisionTreeClassifier.\n",
    "   - What you can say about quality of the best constant model (constant model always predicts one class)\n",
    "   - Train KNN model and evaluate it on `test`. [ Recommended parameters: K=10, euclidian metric ]\n",
    "   - Train KNN for rescaled features. Evaluate quality of the model on `test`. Does that feature transformation increased given metrics for KNN? Repeat experiment for DecisionTree. Why rescaling has no effect on the quality of decision tree?\n",
    "   - Compare all models by all metrics.\n",
    "   - ($\\star$) To find better `train_test_split` you can conduct following experiment: grid search split proportion, for every threshold generate big number of splittings and compute error (according to chosen metric). Then plot dependence of error on split threshold. Choose threshold that gives least variation of the error. Choose metric and plot error standard deviation as function of split threshold. \n",
    "   \n",
    "  Rescaling methods:\n",
    "  - $x_{new} = \\frac{x - \\mu}{\\sigma}$, $\\mu, \\sigma$ — mean, and standard deviation\n",
    "  - $x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$, $[x_{min}, x_{max}]$ — minimal interval of features' values\n",
    "  \n",
    "  As a result of this task you should get following models and compare them between each other: \n",
    "  1. Constant model\n",
    "  2. Decision tree\n",
    "  3. KNN\n",
    "  4. KNN with rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Classification threshold selection\n",
    "\n",
    "Many classification models give estimation of  belonging to the class  $\\tilde{y}(x) \\in R$. Then decision about the class is made by comparing that estimation to some threshold:\n",
    "\n",
    "$y(x) = +1$,  if $\\tilde{y}(x) \\geq \\theta$, $y(x) = -1$, if $\\tilde{y}(x) < \\theta$\n",
    "\n",
    "Confusion matrix and all derivatives (Accuracy, Precision, Recall, etc.) depend on $\\theta$:\n",
    "\n",
    "In order to show the variation of metrics from threshold, you can draw coordinate plane with axis as metrics and the shape of the figure will show you quality of the model.  \n",
    "![ROC-curve construction principle and threshold](https://upload.wikimedia.org/wikipedia/commons/8/8c/Receiver_Operating_Characteristic.png)\n",
    "\n",
    "Most popular curve from such class is ROC-curve (TP-vs-FP plane) and Precision/Recall curves. ROC-curve stands for [Receiver Operating Characteristic](en.wikipedia.org/wiki/Receiver_operating_characteristic).\n",
    "![ROC-curves examples](http://arogozhnikov.github.io/images/roc_curve.gif)\n",
    "\n",
    "[interactive example](http://arogozhnikov.github.io/2015/10/05/roc-curve.html)\n",
    "\n",
    "In the case, if you need to compare quality of classifier without assumptions about threshold, you can use summary statistics or integral metrics based on ROC curve. For example AUC-ROC (**A**rea **U**nder RO**C**) - area under the ROC curve of classifier. AUC-ROC of ideal classifier is 1. Ideal **random** classifier has AUC-ROC about 0.5.\n",
    "\n",
    "\n",
    "#### Task\n",
    "  - Whick classifier has AUC-ROC near 0?\n",
    "  - Is it correct, that classifier from previous part #2 to identify class compare some estimatino with threshold? What are those estimations? What thresholds were chosen?\n",
    "  - For all models from previous task:\n",
    "      - Draw ROC and Precision/Recall curves on the same coordinate plane with different colors. Add legend: which curve corresponds to which classifier.\n",
    "      - Compare AUC-ROC.\n",
    "  - What maximal Recall of spam classification can models achive if you have strict constraint on accuracy: it should not be below 90%?\n",
    "  - ($\\star$) Mark classifiers builded in the previous part with points of different color and shape on the Precision/Recall plane. Add legend: which points correspond to which model. Show isoline of F1 metric (where F1 metric has the same value).\n",
    "  - ($\\star$) Suggest exact effective algorithm to compute AUC-ROC metric with complexity $O(n \\log n)$, where $n$ - number of test examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Cross-validation and parameter selection\n",
    "\n",
    "Each model of machine learning has many structural parameters and parameters of learning: number of neighbours, length function in KNN, max tree depth, mininal number of objects in leafs and so on. There is no universal set of parameters which would be optimal for all given tasks. For every new task you should select other set of parameters.\n",
    "\n",
    "For model's parameters' optimization researchers usually use _grid search_ : pick several values for every parameter, then evaluate every combination of parameters, and choose the best set from the point of optimized metric.\n",
    "\n",
    "If you try a lot of models, it might appear that the best model on `train` did not perserve it's quality on `test` set. We can conclude that in this case model was _overfitted_ to given `train`. \n",
    "\n",
    "To get rid of that problem we can split out dataset on 3 disjoint subsets: `train`, `validation` and `test`\n",
    "![Разбиение на train, validation и test](http://2.bp.blogspot.com/-jkEGMO5lb8A/VmIj1SWT8KI/AAAAAAAAAAs/XTUClFffcX4/s1600/Screen%2BShot%2B2015-12-04%2Bat%2B6.37.34%2BPM.png)\n",
    "\n",
    "`Validation` set is used to compare models when tuning hyperparameters of the model. `Test` is for final assessment of the models.\n",
    "\n",
    "There is more robust method of objective model comparison - [cross-validation](http://en.wikipedia.org/wiki/Cross-validation_(statistics)). \n",
    "There exist different types of that general scheme: \n",
    "   - Leave-One-Out\n",
    "   - K-Fold\n",
    "   - Repeated random sub-sampling\n",
    "\n",
    "Cross-validation is computationally expensive, especially if you do exhaustive grid search with big number of combinations. You should consider following trade-offs:\n",
    "   - you can take more sparse grid, considering less values for each parameter. But you can miss good combination then;\n",
    "   - you can use less partitions or folds, in that case the estimate of cross-validation score becomes more noisy and you have higher risk to make not optimal set of parameters, because of the randomness of partition;\n",
    "   - you can optimize parameters greedily (one by one, consequently), that strategy is not always optimal;\n",
    "   - use random sub-sampling of parameters instead of exhaustive search.\n",
    "   \n",
    "#### Task\n",
    "\n",
    "   - Choose cross-validation method, one from described above. Fixate cross-validation split of `train` set. You should cross-validate on `train` samples from previous tasks, `test` should stay independent. Pay attention: when comparing models, cross-validation split must not change.\n",
    "   - Choose one metric for optimization by grid search. _Example: AUC-ROC._\n",
    "   - Find optimal set of parameters for tree with grid search. Parameters for grid: split criterion, max depth, number of features for node, min number of objects in the leaf (of only some of offered parameters)\n",
    "   - Find optimal set of parameters for KNN using grid search. Parameters for grid: K, metric, weight scheme.\n",
    "   - ($\\star$) [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) -  any test or metric that relies on random sampling with replacement. Can you use different bootstrapping methods for training and evaluating your models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Usefull functions of SciKit-Learn\n",
    "\n",
    "These functions will make compliting this lab a pleasure for you.\n",
    "\n",
    "- Module for quality evaluation and cross-validation [`sklearn.cross_validation`](http://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "    - dataset split [`train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split)\n",
    "    - iterators for cross-validation splits [`LeaveOneOut`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.LeaveOneOut.html#sklearn.cross_validation.LeaveOneOut), [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold), [`ShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.ShuffleSplit.html#sklearn.cross_validation.ShuffleSplit)\n",
    "    - always use `random_state` parameter, it will make your experiments reproducible.\n",
    "- Module with metrics [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "  - [`accuracy_score(y_true, y_pred)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "  - [`precision_recall_fscore_support(y_true, y_pred)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)\n",
    "  - [`roc_curve(y_true, y_score)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)\n",
    "  - [`precision_recall_curve(y_true, y_score)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)\n",
    "  - [`roc_auc_score(y_true, y_score)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)\n",
    "- Module for data preprocessing [`sklearn.preprocessing`](http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "    - function for feature standartization [`scale(X)`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale)\n",
    "    - class that helps to map your features into [0-1] interval [`MinMaxScaler().fit_transform(X)`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "- Module for grid search [`sklearn.grid_search`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.grid_search)\n",
    "    - Class [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV) - implements exhaustive search on a given grid\n",
    "    - Class [`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV) - implements random search of parameters\n",
    "- Class for constant classifier  [`DummyClassifier(strategy='constant', constant=0)`](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier)\n",
    "\n",
    "  \n",
    "### Manual, docs and examples\n",
    "\n",
    "- [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Parameter optimization with cross-val grid search](http://scikit-learn.org/stable/auto_examples/grid_search_digits.html)\n",
    "- [ROC-curves](http://scikit-learn.org/stable/auto_examples/plot_roc.html), [Precision-Recall кривые](http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html)\n",
    "- [Feature importance in decesion trees](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use this jupyter notebook **`magic`** to be able to plot figures directly in notebook.\n",
    "Docs: [`Maplotlib`](http://matplotlib.org/) and [`pylab`](http://wiki.scipy.org/PyLab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spambase dataset downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import urllib2\n",
    "\n",
    "SPAMBASE_NAMES_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'\n",
    "SPAMBASE_DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "\n",
    "feature_names = [\n",
    "    line.strip().split(':')[0] \n",
    "    for line in urllib2.urlopen(SPAMBASE_NAMES_URL).readlines()[33:]\n",
    "]\n",
    "spam_data = pandas.read_csv(SPAMBASE_DATA_URL, header=None, names=(feature_names + ['spam']))\n",
    " \n",
    "X, y = spam_data.drop('spam', 1), spam_data.spam\n",
    " \n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DecisionTree training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris_data = load_iris()\n",
    "X, y = iris_data.data, iris_data.target\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3).fit(X, y)\n",
    "\n",
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Feature importances are calculated based on the frequency of the split by the feature and the depth of the nodes of the tree in which the split took place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "most_important_features = argsort(clf.feature_importances_)[::-1]\n",
    "for idx in most_important_features:\n",
    "    print '%d %s, importance = %.2f' % (idx, iris_data.feature_names[idx], clf.feature_importances_[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Misclassified objects of `train` set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xind, yind = most_important_features[0], most_important_features[1]\n",
    "err = (y != y_pred)\n",
    "scatter(X[err, xind], X[err, yind], c=y_pred[err], marker='x', s=80, linewidths=2)\n",
    "scatter(X[:, xind], X[:, yind], c=y)\n",
    "xlabel(iris_data.feature_names[xind])\n",
    "ylabel(iris_data.feature_names[yind])\n",
    "title(u'Classification errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Tree visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(clf, out_file='tree.dot', feature_names=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to use bash type !command, it might require **conda install graphviz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!dot -Tpng tree.dot -o tree.png\n",
    "from IPython.display import Image\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_data = load_boston()\n",
    "X, y = boston_data.data, boston_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fix 5-Fold partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "cv = KFold(n=len(y), n_folds=5, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pay attention for the arguments:\n",
    "  - `shuffle=True` - it shuffles dataset before partitioning, without it every partition would consist of consequent elements which is bad (you can see that in task #2)\n",
    "  - `random_state` - fixate random state, makes your partition reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Evaluate quality of KNN regression of the Boston Housing dataset for different values of K (number of neighbours). Consider mean absolute error as a quality metric:\n",
    "$$MAE = \\sum_i|y_{pred,i} - y_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's try different `n_neighbors` values, for each value evaluate mean and stdandard deviation of MAE using cross-validation. [Magic `%%time`](http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb) will help you measure time of code execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "k_values = range(1, 10) + range(10, 50, 5)\n",
    "mae_cv_mean = []\n",
    "mae_cv_std = []\n",
    "\n",
    "for k in k_values:\n",
    "    clf = KNeighborsRegressor(n_neighbors=k)\n",
    "    mae_folds = -cross_val_score(clf, X, y, cv=cv, scoring='mean_absolute_error')\n",
    "    mae_cv_mean.append(mae_folds.mean())\n",
    "    mae_cv_std.append(mae_folds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Figure, that describes optimality of parameter selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(8, 4))\n",
    "errorbar(k_values, mae_cv_mean, yerr=array(mae_cv_std)*2)\n",
    "title('Choosing optimal number of neighbors with 5-Fold Cross-Validation')\n",
    "xlabel('n_neighbors')\n",
    "ylabel('mean absolute error')\n",
    "\n",
    "opt_idx = argmin(mae_cv_mean)\n",
    "optimal_k = k_values[opt_idx]\n",
    "optimal_mae = mae_cv_mean[opt_idx]\n",
    "annotate('n_neighbors=%d\\nMAE=%f' % (optimal_k, optimal_mae), \n",
    "         xy=(opt_idx, optimal_mae), xytext=(30, optimal_mae), \n",
    "         arrowprops=dict(facecolor='black', shrink=0.05, alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers, Grishina Anastasiia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spam classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "#### Download dataset [Spambase](https://archive.ics.uci.edu/ml/datasets/Spambase) . (Code is provided bellow)\n",
    "   ##### How many letters in a dataset ?\n",
    "           The dataset covers 4601 letters\n",
    "   ##### Which portion of them is bad (spam) ?\n",
    "           1813 letters are spam\n",
    "   ##### How you can group letters' features ?\n",
    "           by 'word_freq_*' columns [0:47]\n",
    "           by 'char_freq_*' columns [48:53]\n",
    "           by 'capital_run_*' columns [54:56]\n",
    "           by 'spam' column\n",
    "           coulmn numbers are given in the data description of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib2\n",
    "\n",
    "SPAMBASE_NAMES_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'\n",
    "SPAMBASE_DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "\n",
    "feature_names = [\n",
    "    line.strip().split(':')[0] \n",
    "    for line in urllib2.urlopen(SPAMBASE_NAMES_URL).readlines()[33:]\n",
    "]\n",
    "spam_data = pd.read_csv(SPAMBASE_DATA_URL, header=None, names=(feature_names + ['spam']))\n",
    " \n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len (spam_data[ spam_data['spam'] == 1 ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = spam_data.columns.values.tolist()[0:48]\n",
    "char_features = spam_data.columns.values.tolist()[48:54]\n",
    "capital_run_features = spam_data.columns.values.tolist()[54:-1]\n",
    "print ( word_features )\n",
    "print( char_features )\n",
    "print( capital_run_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifier training and it's evaluation\n",
    "  \n",
    "\n",
    "## Split dataset into to disjoint subsets: `train` - first 3000 examples (≈65%), `test` - all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_0 = spam_data[:3000]\n",
    "test_0 = spam_data[3000:]\n",
    "float( train_0.shape[0] )/ float( spam_data.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train decesion tree with `train`. Classify examples from `test`. Calculate classification quality metrics, described above: Accuracy, Precision, Recall, F1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_0, y_0 = train_0.drop('spam', 1), train_0.spam\n",
    "clf_0 = DecisionTreeClassifier( criterion='gini', max_depth=7 ).fit(X_0, y_0)\n",
    "\n",
    "X_pred_0 = test_0.drop('spam', 1)\n",
    "y_pred_0 = clf_0.predict(X_pred_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true_0 = test_0.spam\n",
    "C_0 = confusion_matrix( y_true_0, y_pred_0)\n",
    "def print_conf_m(C):\n",
    "    print ( \" TP = %i \\t FN = %i \\n FP = %i\\t TN = %i\" %( C[1,1], C[1,0], C[0,1], C[0,0] ) )\n",
    "print_conf_m(C_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Accuracy = (TP + TN) / (TP + TN + FP + FN)  — _fraction of correct predictions_\n",
    "  - Precision = TP / (TP + FP)  — _accuracy, fraction of real spam in the letters classified as spam_\n",
    "  - Recall = TP / (TP + FN)  — _completeness, fraction of filtered spam_\n",
    "  - F1 = 2TP / (2TP + FP + FN)  — _harmonic mean of precision and recall_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def find_metrics( y_true, y_pred ):\n",
    "    accuracy = accuracy_score ( y_true, y_pred )\n",
    "    precision = precision_score ( y_true, y_pred )\n",
    "    recall = recall_score ( y_true, y_pred )\n",
    "    f1 = f1_score ( y_true, y_pred )\n",
    "    \n",
    "    print ( \"Accuracy = \\t %5.6f \\nPrecision = \\t %5.6f \\nRecall = \\t %5.6f \\n F1 = \\t\\t %5.6f\" \\\n",
    "           %( accuracy, precision, recall, f1) )\n",
    "    \n",
    "    return ( accuracy, precision, recall, f1 )\n",
    "\n",
    "find_metrics( y_true_0, y_pred_0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which drawbacks does evaluation on this `test` have? How you can make evalution more informative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'test' have no non-zero values in 'spam' feature. To be more informative we should extract another set as 'test' which would have 0 and 1 in spam column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ( test_0.spam[ test_0.spam == 1].count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat experiment with dataset shuffled before `split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # create random index\n",
    "# train_sh = spam_data.sample(n=3000)\n",
    "# test_sh = spam_data.drop(train_sh.index)\n",
    "# test_sh.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in test_sh.index:\n",
    "#     if i in list( train_sh.index ):\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_sh, X_pred_sh, y_sh, y_true_sh = train_test_split( spam_data.drop('spam', 1), spam_data.spam,\\\n",
    "                                                     random_state = 100, test_size = 1601 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_DT_sh = DecisionTreeClassifier( criterion='gini', max_depth=7 ).fit(X_sh, y_sh)\n",
    "\n",
    "y_pred_DT_sh = clf_DT_sh.predict(X_pred_sh)\n",
    "\n",
    "C_DT_sh = confusion_matrix( y_true_sh, y_pred_DT_sh )\n",
    "print_conf_m( C_DT_sh )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_DT_sh = find_metrics ( y_true_sh, y_pred_DT_sh )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which features are the most informative? Use feature importance from DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max inportance of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(\n",
    "    {'Feature': list( spam_data.keys() )[:-1],\n",
    "     'Importance': clf_DT_sh.feature_importances_.tolist()\n",
    "    })\n",
    "\n",
    "importances = importances.sort_values ( 'Importance', ascending = False )\n",
    "print( importances )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you can say about quality of the best constant model (constant model always predicts one class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are 2 constant models possible for the spam dataset: first always predicting spam = 0, second - spam = 1.\n",
    "We have ~40% of all letters are spam while ~60 are not spam. So, the model predicting 0 value for any letter has higher probability of being right for a random letter.\n",
    "\n",
    "--------------------------------------------------\n",
    " \n",
    " So, the metrics` values of the constant model differ as randomly as we choose test and train sets. Accuracy = number of values that the constant model predicts/ number of elements in test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len( spam_data.spam [spam_data.spam == 1].index)/4601."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len( spam_data.spam [spam_data.spam == 0].index)/4601."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "C_Dummy_00 = DummyClassifier( strategy='constant', constant = 0 ).fit(X_sh, y_sh)\n",
    "y_pred_00 = C_Dummy_00.predict(X_pred_sh)\n",
    "y_prob_00 = [ row[1] for row in C_Dummy_00.predict_proba(X_pred_sh) ]\n",
    "\n",
    "C_00 = confusion_matrix( y_true_sh, y_pred_00 )\n",
    "\n",
    "print ( \"Constant model with 0 values\" )\n",
    "print_conf_m( C_00 )\n",
    "metrics_const0 = find_metrics( y_true_sh, y_pred_00 )\n",
    "\n",
    "print('')\n",
    "\n",
    "C_Dummy_11 = DummyClassifier( strategy='constant', constant = 1 ).fit(X_sh, y_sh)\n",
    "y_pred_11 = C_Dummy_11.predict(X_pred_sh)\n",
    "y_prob_11 = [ row[1] for row in C_Dummy_11.predict_proba(X_pred_sh) ]\n",
    "\n",
    "\n",
    "C_11 = confusion_matrix( y_true_sh, y_pred_11 )\n",
    "\n",
    "print ( \"Constant model with 1 values\" )\n",
    "print_conf_m( C_11 )\n",
    "metrics_const1 = find_metrics( y_true_sh, y_pred_11 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( 'Test has %i letters spam = 0 and %i letters spam = 1' \\\n",
    "      %( len( y_true_sh [y_true_sh == 0 ].index), len(y_true_sh [y_true_sh == 1 ].index) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train KNN model and evaluate it on `test`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier has shown worse results then DecisionTreeClassifier on this very sets of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier ( n_neighbors=10, p=2, metric='minkowski' ).fit ( X_sh, y_sh )\n",
    "\n",
    "y_pred_knn = clf_knn.predict(X_pred_sh)\n",
    "y_prob_knn =  [ row[1] for row in clf_knn.predict_proba(X_pred_sh) ]\n",
    "\n",
    "\n",
    "C_knn= confusion_matrix( y_true_sh, y_pred_knn)\n",
    "print_conf_m(C_knn)\n",
    "print( '' )\n",
    "\n",
    "metrics_knn = find_metrics ( y_true_sh, y_pred_knn )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train KNN for rescaled features. Evaluate quality of the model on `test`. Does that feature transformation increased given metrics for KNN? Repeat experiment for DecisionTree. Why rescaling has no effect on the quality of decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Rescale\n",
    "\n",
    "$x_{new} = \\frac{x - \\mu}{\\sigma}$, $\\mu, \\sigma$ — mean, and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_rescale_1 = spam_data.copy()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_r = spam_rescale_1.iloc[X_sh.index]\n",
    "test_r  = spam_rescale_1.iloc[X_pred_sh.index]\n",
    "\n",
    "scaler = StandardScaler( ).fit ( train_r.drop('spam', 1) )\n",
    "X_r = pd.DataFrame(scaler.transform( train_r.drop('spam', 1) ) , index=train_r.index, columns=train_r.drop('spam', 1).columns)\n",
    "y_r = train_r.spam\n",
    "X_pred_r = scaler.transform( test_r.drop('spam', 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_knn_r = KNeighborsClassifier ( n_neighbors=10, p=2, metric='minkowski' ).fit ( X_r, y_r )\n",
    "\n",
    "y_pred_knn_r = clf_knn_r.predict(X_pred_r)\n",
    "y_prob_knn_r = [ row[1] for row in clf_knn_r.predict_proba(X_pred_r) ]\n",
    "\n",
    "C_knn_r = confusion_matrix( y_true_sh, y_pred_knn_r)\n",
    "print_conf_m (C_knn_r)\n",
    "print( '' )\n",
    "\n",
    "metrics_knn_r = find_metrics( y_true_sh, y_pred_knn_r )\n",
    "print( metrics_knn_r )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_DT_r = DecisionTreeClassifier( criterion='gini', max_depth=7 ).fit(X_r, y_r)\n",
    "\n",
    "y_pred_DT_r = clf_DT_r.predict(X_pred_r)\n",
    "y_prob_DT_r = [ row[1] for row in  clf_DT_r.predict_proba(X_pred_r) ]\n",
    "\n",
    "\n",
    "C_DT_r = confusion_matrix( y_true_sh, y_pred_DT_r )\n",
    "print_conf_m (C_DT_r)\n",
    "\n",
    "print( '' )\n",
    "\n",
    "metrics_DT_r = find_metrics( y_true_sh, y_pred_DT_r )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree: Metrics for rescaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( metrics_DT_r )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree: Metrics for initial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ( metrics_DT_sh )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling does not play role for Decision Tree classifier due to the rules of the model composition. Rescaling does not influence the structure of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare all models by all metrics.\n",
    "\n",
    "  As a result of this task you should get following models and compare them between each other: \n",
    "  1. Constant model\n",
    "  2. Decision tree\n",
    "  3. KNN\n",
    "  4. KNN with rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_comp = pd.DataFrame( 0, index=['Const0', 'Const1', 'Decision Tree', 'KNN',\\\n",
    "                                     'KNN rescale mean' ],\\\n",
    "                         columns = ['Accuracy', 'Precision', 'Recall', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_comp.ix['Const0',:] = metrics_const0\n",
    "model_comp.ix['Const1',:] = metrics_const1\n",
    "model_comp.ix['Decision Tree',:] = metrics_DT_sh\n",
    "model_comp.ix['KNN',:] = metrics_knn\n",
    "model_comp.ix['KNN rescale mean',:] = metrics_knn_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ($\\star$) To find better `train_test_split` you can conduct following experiment: grid search split proportion, for every threshold generate big number of splittings and compute error (according to chosen metric). Then plot dependence of error on split threshold. Choose threshold that gives least variation of the error. Choose metric and plot error standard deviation as function of split threshold. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification threshold selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  - ($\\star$) Mark classifiers builded in the previous part with points of different color and shape on the Precision/Recall plane. Add legend: which points correspond to which model. Show isoline of F1 metric (where F1 metric has the same value).\n",
    "  - ($\\star$) Suggest exact effective algorithm to compute AUC-ROC metric with complexity $O(n \\log n)$, where $n$ - number of test examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which classifier has AUC-ROC near 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a classifier that almost always predicts wrong values \n",
    "\n",
    "(e.g. true = 1, but this classifier predicts = 0; \n",
    " \n",
    " true = 0, pred = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it correct, that classifier from previous part #2 to identify class compare some estimatino with threshold? What are those estimations? What thresholds were chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is correct for the Decision Tree. \n",
    "\n",
    "The thresholds are given in Decision Tree visualisation and in thresholds below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz( clf_DT_sh, out_file='tree.dot', feature_names=spam_data.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_DT_sh.tree_.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For all models from previous task:\n",
    "  #### Draw ROC and Precision/Recall curves on the same coordinate plane with different colors. Add legend: which curve corresponds to which classifier.\n",
    "  #### Compare AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_comp.insert ( len(model_comp.keys()), \"AUC\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type( y_prob_DT_r )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr_00, tpr_00, thresholds_00 = metrics.roc_curve(y_true_sh, y_prob_00)\n",
    "model_comp.ix[0, 'AUC'] = metrics.auc(fpr_00, tpr_00)\n",
    "\n",
    "fpr1, tpr1, thresholds_11 = metrics.roc_curve(y_true_sh, y_prob_11)\n",
    "model_comp.ix[1, 'AUC'] = metrics.auc(fpr1, tpr1)\n",
    "\n",
    "fpr_DT, tpr_DT, thresholds_DT = metrics.roc_curve(y_true_sh, y_prob_DT_r)\n",
    "model_comp.ix[2, 'AUC'] = metrics.auc(fpr_DT, tpr_DT)\n",
    "\n",
    "fpr_knn, tpr_knn, thresholds_knn = metrics.roc_curve(y_true_sh, y_prob_knn)\n",
    "model_comp.ix[3, 'AUC'] = metrics.auc(fpr_knn, tpr_knn)\n",
    "\n",
    "fpr_knn_r, tpr_knn_r, thresholds_knn_r = metrics.roc_curve(y_true_sh, y_prob_knn_r)\n",
    "model_comp.ix[4, 'AUC'] = metrics.auc(fpr_knn_r, tpr_knn_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = ('Const0', 'Const1', 'Decision Tree','KNN','KNN rescale mean')\n",
    "x_pos = np.arange( len(x) )\n",
    "y = model_comp['AUC']\n",
    " \n",
    "plt.bar(x_pos, y.values, align='center', alpha=0.5, color=('b','orange','r','g','purple'))\n",
    "plt.xticks(x_pos, x, rotation = 90)\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Models')\n",
    "plt.title('AUC ROC comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 1.5\n",
    "\n",
    "plt.plot(fpr_00, tpr_00, color='r',\n",
    "         lw=lw, label='Constant model 0. AUC = %1.2f' % model_comp.ix[0, 'AUC'])\n",
    "\n",
    "plt.plot(fpr1, tpr1, color='b',\n",
    "         lw=lw, label='Constant model 1. AUC = %1.2f' % model_comp.ix[1, 'AUC'])\n",
    "\n",
    "plt.plot(fpr_DT, tpr_DT, color='g',\n",
    "         lw=lw, label='Decision Tree. AUC = %1.2f' % model_comp.ix[2, 'AUC'])\n",
    "\n",
    "plt.plot(fpr_knn, tpr_knn, color='k',\n",
    "         lw=lw, label='KNN. AUC = %1.2f' % model_comp.ix[3, 'AUC'])\n",
    "\n",
    "plt.plot(fpr_knn_r, tpr_knn_r, color='purple',\n",
    "         lw=lw, label='KNN Rescaling. AUC = %1.2f' % model_comp.ix[4, 'AUC'])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='white', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for the models')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute pairs of precision and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prec_00, recall_00, thresholds_00 = metrics.precision_recall_curve(y_true_sh, y_prob_00)\n",
    "prec_11, recall_11, thresholds_11 = metrics.precision_recall_curve(y_true_sh, y_prob_11)\n",
    "prec_DT, recall_DT, thresholds_DT = metrics.precision_recall_curve(y_true_sh, y_prob_DT_r)\n",
    "prec_knn, recall_knn, thresholds_knn = metrics.precision_recall_curve(y_true_sh, y_prob_knn)\n",
    "prec_knn_r, recall_knn_r, thresholds_knn_r = metrics.precision_recall_curve(y_true_sh, y_prob_knn_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 1.5\n",
    "\n",
    "plt.plot(recall_00, prec_00, color='r',\n",
    "         lw=lw, label='Constant model 0' )\n",
    "\n",
    "plt.plot(recall_11, prec_11, color='b',\n",
    "         lw=lw, label='Constant model 1' )\n",
    "\n",
    "plt.plot(recall_DT, prec_DT, color='g',\n",
    "         lw=lw, label='Decision Tree' )\n",
    "\n",
    "plt.plot(recall_knn, prec_knn, color='k',\n",
    "         lw=lw, label='KNN' )\n",
    "\n",
    "plt.plot(recall_knn_r, prec_knn_r, color='purple',\n",
    "         lw=lw, label='KNN Rescaling' )\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision/Recall Curves')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What maximal Recall of spam classification can models acheive if you have strict constraint on accuracy: it should not be below 90%?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Precision_recall_curve: we will find the intersection of the line y = 0.9 and of the Precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recall_when_precision_09 ( prec, recall ):\n",
    "    ix = np.where( prec > 0.9 )[0]\n",
    "    ix = ix[0]\n",
    "    \n",
    "    p_l = [ recall[ix - 1], prec[ix - 1] ]\n",
    "    p_r = [ recall[ix], prec[ix] ]\n",
    "\n",
    "    a = ( p_l[1] - p_r[1] )/ ( p_l[0] - p_r[0] )\n",
    "    b = p_l[1] - a * p_l[0]\n",
    "\n",
    "    x = ( 0.9 - b )/a\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_comp.insert ( len(model_comp.keys()), \"Max Recall, Precision>=0.9\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recalls_val = ( recall_when_precision_09( prec_00, recall_00),\\\n",
    "               recall_when_precision_09( prec_11, recall_11),\\\n",
    "               recall_when_precision_09( prec_DT, recall_DT),\\\n",
    "               recall_when_precision_09( prec_knn, recall_knn),\\\n",
    "               recall_when_precision_09( prec_knn_r, recall_knn_r) )\n",
    "\n",
    "model_comp.ix[:,5] = recalls_val\n",
    "# model_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = ('Const0', 'Const1', 'Decision Tree','KNN','KNN rescale mean')\n",
    "x_pos = np.arange( len(x) )\n",
    "y = model_comp.iloc[:,5]\n",
    " \n",
    "plt.bar(x_pos, y.values, align='center', alpha=0.5, color=('b','orange','r','g','purple'))\n",
    "plt.xticks(x_pos, x, rotation = 90)\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Models')\n",
    "plt.title('Max Recall when Precision >= 0.9')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-validation and parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Choose cross-validation method, one from described above. Fixate cross-validation split of `train` set. You should cross-validate on `train` samples from previous tasks, `test` should stay independent. Pay attention: when comparing models, cross-validation split must not change.\n",
    "   - Choose one metric for optimization by grid search. _Example: AUC-ROC._\n",
    "   - Find optimal set of parameters for tree with grid search. Parameters for grid: split criterion, max depth, number of features for node, min number of objects in the leaf (of only some of offered parameters)\n",
    "   - Find optimal set of parameters for KNN using grid search. Parameters for grid: K, metric, weight scheme.\n",
    "   - ($\\star$) [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) -  any test or metric that relies on random sampling with replacement. Can you use different bootstrapping methods for training and evaluating your models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated random sub-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal set of parameters for tree with grid search. Parameters for grid: split criterion, max depth, number of features for node, min number of objects in the leaf (of only some of offered parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'max_depth': ( 2,4,8,10,12,14,16 ),\n",
    "    'max_features': ( 'auto', 'sqrt', 'log2' ),\n",
    "    'min_samples_leaf': (1,2,3,4,5,7,10,13,15 )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf_grid_s = RandomizedSearchCV ( DecisionTreeClassifier(), parameters, scoring='roc_auc', n_jobs=8, cv=5 )\n",
    "clf_grid_s.fit ( X_r, y_r )\n",
    "\n",
    "print ( \"Parameters:\", clf_grid_s.best_params_ )\n",
    "print ( \"Score:\", clf_grid_s.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_knn = {\n",
    "    'n_neighbors': [5,7,10,12,15,17,20],\n",
    "    'metric': ( 'minkowski', 'manhattan', 'chebyshev', 'euclidean' ),\n",
    "    'weights': ( 'uniform', 'distance' )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "clf_grid_s_knn = RandomizedSearchCV ( KNeighborsClassifier(), parameters_knn, scoring='roc_auc', n_jobs=7, cv=5 )\n",
    "clf_grid_s_knn.fit ( X_r, y_r )\n",
    "\n",
    "\n",
    "print ( \"Parameters:\", clf_grid_s_knn.best_params_ )\n",
    "print ( \"Score:\", clf_grid_s_knn.best_score_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing last remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Instead of \n",
    "X_0, y_0 = train_0.ix[:, :-1].values, train_0.ix[:, -1].values\n",
    "USE\n",
    "X_0, y_0 = train_0.drop('Spam'), train_0['Spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_0, y_0 = train_0.drop('spam', 1), train_0.spam\n",
    "clf_0 = DecisionTreeClassifier( criterion='gini', max_depth=7 ).fit(X_0, y_0)\n",
    "\n",
    "X_pred_0 = test_0.drop('spam', 1)\n",
    "y_pred_0 = clf_0.predict(X_pred_0)\n",
    "\n",
    "# and evrywhere further in he same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. when you use that train and did not change test. you almost sure has some same objects in train and test.\n",
    "train_sh = spam_data.sample(n=3000)\n",
    "test_sh = spam_data.drop(train_sh.index)\n",
    "test_sh.shape[0]\n",
    "\n",
    "USE train_test_split from sklean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_sh, X_pred_sh, y_sh, y_true_sh = train_test_split( spam_data.drop('spam', 1), spam_data.spam,\\\n",
    "                                                     random_state = 100, test_size = 1601 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. it is better to output features sorted by importance, than just output the most important. Because the question was in plural form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(\n",
    "    {'Feature': list( spam_data.keys() )[:-1],\n",
    "     'Importance': clf_DT_sh.feature_importances_.tolist()\n",
    "    })\n",
    "\n",
    "importances = importances.sort_values ( 'Importance', ascending = False )\n",
    "print( importances )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. you can pass whole dataframe to scaler, be careful, you can not use info from test for scaling.\n",
    "\n",
    "#### changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_rescale_1 = spam_data.copy()\n",
    "\n",
    "train_r = spam_rescale_1.iloc[X_sh.index]\n",
    "test_r  = spam_rescale_1.iloc[X_pred_sh.index]\n",
    "\n",
    "scaler = StandardScaler( ).fit ( train_r.drop('spam', 1) )\n",
    "X_r = pd.DataFrame(scaler.transform( train_r.drop('spam', 1) ) , index=train_r.index, columns=train_r.drop('spam', 1).columns)\n",
    "y_r = train_r.spam\n",
    "X_pred_r = scaler.transform( test_r.drop('spam', 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. it is hard to compare floats, use pictures \n",
    "\n",
    "#### changes: added figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = ('Const0', 'Const1', 'Decision Tree','KNN','KNN rescale mean')\n",
    "x_pos = np.arange( len(x) )\n",
    "y = model_comp['AUC']\n",
    " \n",
    "plt.bar(x_pos, y.values, align='center', alpha=0.5, color=('b','orange','r','g','purple'))\n",
    "plt.xticks(x_pos, x, rotation = 90)\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Models')\n",
    "plt.title('AUC ROC comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = ('Const0', 'Const1', 'Decision Tree','KNN','KNN rescale mean')\n",
    "x_pos = np.arange( len(x) )\n",
    "y = model_comp.iloc[:,5]\n",
    " \n",
    "plt.bar(x_pos, y.values, align='center', alpha=0.5, color=('b','orange','r','g','purple'))\n",
    "plt.xticks(x_pos, x, rotation = 90)\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Models')\n",
    "plt.title('Max Recall when Precision >= 0.9')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. which sklearn version are you using? you can find it from terminal `conda list` or from code sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After your remarks regarding the threshold question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it correct, that classifier from previous part #2 to identify class compare some estimatino with threshold? What are those estimations? What thresholds were chosen?\n",
    "\n",
    "Answer: \n",
    "\n",
    "Yes, not only Decision Tree, but all the classification algorythms have their own final thresholds. If the result of 'predict.proba' > threshold(or probability of being a number of the specific class, e.g. class 1) then 'predict' denotes the object to the class 1, otherwise - to another one."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
